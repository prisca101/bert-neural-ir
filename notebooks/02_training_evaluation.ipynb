{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adzoqlAAnSAY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option('display.max_rows', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Bj4m7hlfHxk"
      },
      "outputs": [],
      "source": [
        "pd.reset_option('display.max_colwidth')\n",
        "pd.reset_option('display.max_rows')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rME4LURodxA2",
        "outputId": "fd1bd051-c01a-45cf-d703-0d322e50e87a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m866.1/866.1 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.0/149.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.6/45.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.8/304.8 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for warc3-wet-clueweb09 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for cbor (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# =============================================\n",
        "# Complete pipeline up to XGBoost feature preparation\n",
        "# NFCorpus (BEIR) - BM25 + BERT reranker features\n",
        "# Runs in free Colab (~5-15 min depending on GPU/CPU)\n",
        "# =============================================\n",
        "\n",
        "# 0. Installs\n",
        "!pip install -q ir_datasets rank_bm25 sentence-transformers xgboost ir_measures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FocFTSUCdMc6"
      },
      "outputs": [],
      "source": [
        "import ir_datasets\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import CrossEncoder\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import torch\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4vF4ndpa7zd",
        "outputId": "6a29b90e-7fdc-417d-d54c-c089299f49b4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[INFO] [starting] building docstore\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading NFCorpus...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[INFO] [starting] opening zip file\n",
            "[INFO] [starting] https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/nfcorpus.zip\n",
            "docs_iter:   0%|                                      | 0/3633 [00:01<?, ?doc/s]\n",
            "https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/nfcorpus.zip: 0.0%| 0.00/2.45M [00:00<?, ?B/s]\u001b[A\n",
            "https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/nfcorpus.zip: 1.3%| 32.8k/2.45M [00:00<00:10, 232kB/s]\u001b[A\n",
            "https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/nfcorpus.zip: 3.3%| 81.9k/2.45M [00:00<00:08, 284kB/s]\u001b[A\n",
            "https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/nfcorpus.zip: 8.0%| 197k/2.45M [00:00<00:05, 450kB/s] \u001b[A\n",
            "https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/nfcorpus.zip: 16.7%| 410k/2.45M [00:00<00:02, 704kB/s]\u001b[A\n",
            "https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/nfcorpus.zip: 31.5%| 770k/2.45M [00:00<00:01, 1.06MB/s]\u001b[A\n",
            "\n",
            "\u001b[A[INFO] [finished] https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/nfcorpus.zip: [00:01] [2.45MB] [2.36MB/s]\n",
            "docs_iter:   0%|                                      | 0/3633 [00:02<?, ?doc/s]\n",
            "https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/nfcorpus.zip: [00:01] [2.45MB] [2.32MB/s]\u001b[A\n",
            "[INFO] [finished] opening zip file [2.14s]\n",
            "docs_iter: 100%|█████████████████████████| 3633/3633 [00:02<00:00, 1390.33doc/s]\n",
            "[INFO] [finished] docs_iter: [00:02] [3633doc] [1389.95doc/s]\n",
            "[INFO] [finished] building docstore [2.62s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Corpus size: 3633 documents\n"
          ]
        }
      ],
      "source": [
        "# ========================\n",
        "# 0. Load NFCorpus (train for index, test for evaluation prep)\n",
        "# ========================\n",
        "print(\"Loading NFCorpus...\")\n",
        "dataset_train = ir_datasets.load(\"beir/nfcorpus/train\")\n",
        "dataset_test  = ir_datasets.load(\"beir/nfcorpus/test\")\n",
        "\n",
        "# Corpus is the same across splits — we use full corpus text\n",
        "all_docs = list(ir_datasets.load(\"beir/nfcorpus\").docs_iter())  # or train.docs_iter()\n",
        "doc_id_to_text = {doc.doc_id: doc.text for doc in all_docs}\n",
        "doc_ids = list(doc_id_to_text.keys())\n",
        "\n",
        "print(f\"Corpus size: {len(doc_ids)} documents\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZ4Qq1AybKWG"
      },
      "source": [
        "## Training and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cG2O6l62lQWn",
        "outputId": "05986c26-27f8-4a96-ceda-198976fea313"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading NFCorpus...\n",
            "Corpus size: 3633 documents\n"
          ]
        }
      ],
      "source": [
        "# ========================\n",
        "# 0. Load NFCorpus (train for index, test for evaluation prep)\n",
        "# ========================\n",
        "print(\"Loading NFCorpus...\")\n",
        "dataset_train = ir_datasets.load(\"beir/nfcorpus/train\")\n",
        "dataset_test  = ir_datasets.load(\"beir/nfcorpus/test\")\n",
        "\n",
        "# Corpus is the same across splits — we use full corpus text\n",
        "all_docs = list(ir_datasets.load(\"beir/nfcorpus\").docs_iter())\n",
        "doc_id_to_text = {doc.doc_id: doc.text for doc in all_docs}\n",
        "doc_ids = list(doc_id_to_text.keys())\n",
        "\n",
        "print(f\"Corpus size: {len(doc_ids)} documents\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlbIM1HDXmq7",
        "outputId": "9c55143f-2e0d-42c0-bf1a-71efb6d934b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Kept 2045 out of 2590 train queries\n",
            "Downsampled 335 queries to 64 positives\n"
          ]
        }
      ],
      "source": [
        "# ───────────────────────────────────────────────────────────────\n",
        "# Filter Qrels\n",
        "# ───────────────────────────────────────────────────────────────\n",
        "\n",
        "# 1. Build qrels_dict for TRAIN\n",
        "train_qrels_dict = defaultdict(list)\n",
        "for qrel in dataset_train.qrels_iter():\n",
        "    train_qrels_dict[qrel.query_id].append(qrel.doc_id)\n",
        "\n",
        "# 2. Apply your filtering\n",
        "MIN_REL = 5\n",
        "MAX_REL = 64\n",
        "random.seed(42)\n",
        "\n",
        "valid_train_qids = {\n",
        "    qid for qid, docs in train_qrels_dict.items()\n",
        "    if MIN_REL <= len(docs)\n",
        "}\n",
        "\n",
        "filtered_train_qrels = {}\n",
        "downsampled = 0\n",
        "\n",
        "for qid in valid_train_qids:\n",
        "    docs = train_qrels_dict[qid]\n",
        "    if len(docs) > MAX_REL:\n",
        "        docs = random.sample(docs, MAX_REL)\n",
        "        downsampled += 1\n",
        "    filtered_train_qrels[qid] = set(docs)   # set for fast .in check later\n",
        "\n",
        "print(f\"Kept {len(filtered_train_qrels)} out of {len(train_qrels_dict)} train queries\")\n",
        "print(f\"Downsampled {downsampled} queries to {MAX_REL} positives\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 743,
          "referenced_widgets": [
            "97e9a6992d624806bb8ab6fde93cb09a",
            "f23457510032417c932208452e88c6dc",
            "1a85fde2621e46f5b3d629438a229a66",
            "b7407824d18f40f5b6adefb6b320bc77",
            "ceceac60e77145fca5d6d1a852816c7d",
            "b2fde80be0e44675a765c9df2c0c3388",
            "976989d10af04dd2afaf6a6e358a915c",
            "aff095f64c42451ebf8e602d866a98eb",
            "8f6a93e812f24acda0b3e503b8f3913b",
            "46445ad693d14885a7be60c0ba12316d",
            "b95d72ffe9a045afb8a57c6204a42cb1"
          ]
        },
        "id": "atI6deQieJC7",
        "outputId": "60ec395a-5a51-4f17-c2b1-195ed73149ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading NFCorpus...\n",
            "Corpus size: 3633 documents\n",
            "Building BM25 index...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing corpus: 100%|██████████| 3633/3633 [00:00<00:00, 24095.89it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BM25 index ready.\n",
            "Loading Cross-Encoder reranker...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "97e9a6992d624806bb8ab6fde93cb09a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/105 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "BertForSequenceClassification LOAD REPORT from: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
            "Key                          | Status     |  | \n",
            "-----------------------------+------------+--+-\n",
            "bert.embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reranker loaded.\n",
            "Processing 2590 train queries...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing queries: 100%|██████████| 2590/2590 [1:07:44<00:00,  1.57s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Feature DataFrame ready for XGBoost LTR:\n",
            "       qid    doc_id  bm25_score  bert_score  doc_length  query_length  label  \\\n",
            "0  PLAIN-3  MED-2427    1.000000   -2.284123         149             6      1   \n",
            "1  PLAIN-3  MED-2774    0.783629   -5.930433         144             6      0   \n",
            "2  PLAIN-3  MED-2434    0.718583    2.986796         236             6      1   \n",
            "3  PLAIN-3  MED-1180    0.605856   -6.064842         238             6      0   \n",
            "4  PLAIN-3  MED-4465    0.601393   -6.920015         140             6      0   \n",
            "5  PLAIN-3  MED-3551    0.599772   -5.907237         207             6      0   \n",
            "6  PLAIN-3  MED-2102    0.592494   -3.955021         209             6      0   \n",
            "7  PLAIN-3  MED-5087    0.583675   -7.183592         222             6      0   \n",
            "\n",
            "     group  \n",
            "0  PLAIN-3  \n",
            "1  PLAIN-3  \n",
            "2  PLAIN-3  \n",
            "3  PLAIN-3  \n",
            "4  PLAIN-3  \n",
            "5  PLAIN-3  \n",
            "6  PLAIN-3  \n",
            "7  PLAIN-3  \n",
            "\n",
            "Total rows (candidates): 777000\n",
            "Number of queries: 2590\n"
          ]
        }
      ],
      "source": [
        "# ========================\n",
        "# 0. Load NFCorpus (train for index, test for evaluation prep)\n",
        "# ========================\n",
        "print(\"Loading NFCorpus...\")\n",
        "dataset_train = ir_datasets.load(\"beir/nfcorpus/train\")\n",
        "dataset_test  = ir_datasets.load(\"beir/nfcorpus/test\")\n",
        "\n",
        "# Corpus is the same across splits — we use full corpus text\n",
        "all_docs = list(ir_datasets.load(\"beir/nfcorpus\").docs_iter())  # or train.docs_iter()\n",
        "doc_id_to_text = {doc.doc_id: doc.text for doc in all_docs}\n",
        "doc_ids = list(doc_id_to_text.keys())\n",
        "\n",
        "print(f\"Corpus size: {len(doc_ids)} documents\")\n",
        "\n",
        "# ========================\n",
        "# 1. Build BM25 index\n",
        "# ========================\n",
        "print(\"Building BM25 index...\")\n",
        "tokenized_corpus = [doc.text.lower().split() for doc in tqdm(all_docs, desc=\"Tokenizing corpus\")]\n",
        "bm25 = BM25Okapi(tokenized_corpus)\n",
        "\n",
        "print(\"BM25 index ready.\")\n",
        "\n",
        "# ========================\n",
        "# 2. Load fast Cross-Encoder reranker\n",
        "# ========================\n",
        "print(\"Loading Cross-Encoder reranker...\")\n",
        "reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2', device='cuda')  # use 'cpu' if no GPU\n",
        "print(\"Reranker loaded.\")\n",
        "\n",
        "# ========================\n",
        "# 3. For each test query → BM25 top-100 → BERT scores → build features\n",
        "# ========================\n",
        "\n",
        "# We will collect ALL rows across all queries\n",
        "feature_rows = []\n",
        "qid_list = []           # for XGBoost group (query id repeated for each candidate)\n",
        "relevance_list = []     # ground truth labels\n",
        "\n",
        "# Map query_id → relevance dict for fast lookup\n",
        "qrels_dict = {}\n",
        "\n",
        "for qrel in dataset_train.qrels_iter():\n",
        "    qid = qrel.query_id\n",
        "    did = qrel.doc_id\n",
        "    rel = qrel.relevance\n",
        "    if qid not in qrels_dict:\n",
        "        qrels_dict[qid] = {}\n",
        "    qrels_dict[qid][did] = rel\n",
        "\n",
        "train_queries = list(dataset_train.queries_iter())\n",
        "\n",
        "print(f\"Processing {len(train_queries)} train queries...\")\n",
        "\n",
        "for query in tqdm(train_queries, desc=\"Processing queries\"):\n",
        "    qid = query.query_id\n",
        "    qtext = query.text\n",
        "\n",
        "    # 3.1 BM25 retrieve top-100\n",
        "    tokenized_query = qtext.lower().split()\n",
        "    bm25_scores = bm25.get_scores(tokenized_query)\n",
        "\n",
        "    # Get top-100 doc indices & scores\n",
        "    top_idx = np.argsort(bm25_scores)[::-1][:300]\n",
        "    top_doc_ids = [doc_ids[i] for i in top_idx]\n",
        "    top_bm25_scores = bm25_scores[top_idx]\n",
        "\n",
        "    # Normalize BM25 scores a bit (optional but helps)\n",
        "    if len(top_bm25_scores) > 0:\n",
        "        top_bm25_scores = (top_bm25_scores - top_bm25_scores.min()) / (top_bm25_scores.max() - top_bm25_scores.min() + 1e-8)\n",
        "\n",
        "    # 3.2 Prepare pairs for cross-encoder\n",
        "    pairs = [[qtext, doc_id_to_text[did]] for did in top_doc_ids]\n",
        "\n",
        "    # 3.3 Get BERT (cross-encoder) scores\n",
        "    with torch.no_grad():\n",
        "        bert_scores = reranker.predict(pairs, batch_size=32, show_progress_bar=False)\n",
        "\n",
        "    # 3.4 Build features for each candidate\n",
        "    for rank_in_top, did in enumerate(top_doc_ids):\n",
        "        bm25_score = top_bm25_scores[rank_in_top]\n",
        "        bert_score = bert_scores[rank_in_top]\n",
        "\n",
        "        # Simple extra features (you can add more!)\n",
        "        doc_len = len(doc_id_to_text[did].split())\n",
        "        query_len = len(tokenized_query)\n",
        "\n",
        "        feature_rows.append({\n",
        "            'qid': qid,\n",
        "            'doc_id': did,\n",
        "            'bm25_score': bm25_score,\n",
        "            'bert_score': bert_score,\n",
        "            'doc_length': doc_len,\n",
        "            'query_length': query_len,\n",
        "            # rank_in_bm25: rank_in_top,   # you can add if you want position bias feature\n",
        "        })\n",
        "\n",
        "        # Ground truth label\n",
        "        rel = qrels_dict.get(qid, {}).get(did, 0)\n",
        "        relevance_list.append(rel)\n",
        "\n",
        "    # For XGBoost group sizes\n",
        "    qid_list.extend([qid] * len(top_doc_ids))\n",
        "\n",
        "# ========================\n",
        "# Convert to DataFrames\n",
        "# ========================\n",
        "\n",
        "features_df = pd.DataFrame(feature_rows)\n",
        "features_df['label'] = relevance_list\n",
        "features_df['group'] = qid_list   # repeated qid for each candidate of the query\n",
        "\n",
        "print(\"\\nFeature DataFrame ready for XGBoost LTR:\")\n",
        "print(features_df.head(8))\n",
        "print(f\"\\nTotal rows (candidates): {len(features_df)}\")\n",
        "print(f\"Number of queries: {features_df['qid'].nunique()}\")\n",
        "\n",
        "# Example columns you now have:\n",
        "# qid, doc_id, bm25_score, bert_score, doc_length, query_length, label, group"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOUyxqxxPgoM",
        "outputId": "19f088b0-2930-43a3-8dac-4fd2b2683431"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train candidates: 621,600  |  Queries: 2072\n",
            "  Val candidates: 155,400  |  Queries: 518\n",
            "Pre-extracted val qrels: 24551 judgments over 518 queries\n",
            "BM25_only    → train shape (621600, 3), val shape (155400, 3)\n",
            "BERT_only    → train shape (621600, 3), val shape (155400, 3)\n",
            "Combined     → train shape (621600, 4), val shape (155400, 4)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "import numpy as np\n",
        "\n",
        "# ─── 1. Split by query groups ───\n",
        "splitter = GroupShuffleSplit(\n",
        "    n_splits=1,\n",
        "    test_size=0.2,          # → 80% train / 20% val by number of queries\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# groups = qid array (one qid per row/candidate)\n",
        "train_idx, val_idx = next(splitter.split(\n",
        "    X=np.arange(len(features_df)),  # dummy X, we only need indices\n",
        "    groups=features_df['qid'].values\n",
        "))\n",
        "\n",
        "# ─── 2. Create train / val dataframes ───\n",
        "df_train = features_df.iloc[train_idx].copy()\n",
        "df_val   = features_df.iloc[val_idx].copy()\n",
        "\n",
        "# Optional: reset index if you want clean indexing\n",
        "df_train = df_train.reset_index(drop=True)\n",
        "df_val   = df_val.reset_index(drop=True)\n",
        "\n",
        "print(f\"Train candidates: {len(df_train):,d}  |  Queries: {df_train['qid'].nunique()}\")\n",
        "print(f\"  Val candidates: {len(df_val):,d}  |  Queries: {df_val['qid'].nunique()}\")\n",
        "\n",
        "\n",
        "# ─── Prepare validation qrels (only judgments that appear in val set) ───\n",
        "# This is important — we only evaluate on queries & documents that are in validation\n",
        "val_qids_set = set(df_val['qid'].unique())\n",
        "\n",
        "# Extract validation qrels once (from train split!)\n",
        "val_qrels = []\n",
        "for qrel in dataset_train.qrels_iter():\n",
        "    if qrel.query_id in val_qids_set:\n",
        "        val_qrels.append(ir_measures.Qrel(\n",
        "            query_id = qrel.query_id,\n",
        "            doc_id = qrel.doc_id,\n",
        "            relevance = qrel.relevance,\n",
        "        ))\n",
        "\n",
        "print(f\"Pre-extracted val qrels: {len(val_qrels)} judgments over {len(val_qids_set)} queries\")\n",
        "\n",
        "# ─── 3. Define the feature column groups you want to compare ───\n",
        "feature_sets = {\n",
        "    \"BM25_only\":   ['bm25_score', 'doc_length', 'query_length'],\n",
        "    \"BERT_only\":   ['bert_score', 'doc_length', 'query_length'],\n",
        "    \"Combined\":    ['bm25_score', 'bert_score', 'doc_length', 'query_length'],\n",
        "    # you can add more variants here later\n",
        "}\n",
        "\n",
        "# ─── 4. Prepare train / val matrices for each variant ───\n",
        "X_train_dict = {}\n",
        "X_val_dict   = {}\n",
        "y_train = df_train['label'].values.astype(np.float32)\n",
        "y_val   = df_val['label'].values.astype(np.float32)\n",
        "\n",
        "group_train = df_train.groupby('qid').size().to_numpy()\n",
        "group_val   = df_val.groupby('qid').size().to_numpy()\n",
        "\n",
        "for name, cols in feature_sets.items():\n",
        "    X_train_dict[name] = df_train[cols].values\n",
        "    X_val_dict[name]   = df_val[cols].values\n",
        "    print(f\"{name:12} → train shape {X_train_dict[name].shape}, val shape {X_val_dict[name].shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_r3C3YoxPmsm",
        "outputId": "1966a245-4364-4a7e-9d91-d93fabe5450e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training BM25_only ...\n",
            "[0]\tvalidation_0-ndcg@10:0.53531\n",
            "[20]\tvalidation_0-ndcg@10:0.53893\n",
            "[40]\tvalidation_0-ndcg@10:0.53761\n",
            "[60]\tvalidation_0-ndcg@10:0.53834\n",
            "[62]\tvalidation_0-ndcg@10:0.53773\n",
            "Best iteration: 22, best ndcg@10: 0.5394\n",
            "\n",
            "Training BERT_only ...\n",
            "[0]\tvalidation_0-ndcg@10:0.59937\n",
            "[20]\tvalidation_0-ndcg@10:0.60272\n",
            "[40]\tvalidation_0-ndcg@10:0.60153\n",
            "[60]\tvalidation_0-ndcg@10:0.59913\n",
            "[68]\tvalidation_0-ndcg@10:0.60000\n",
            "Best iteration: 28, best ndcg@10: 0.6038\n",
            "\n",
            "Training Combined ...\n",
            "[0]\tvalidation_0-ndcg@10:0.59072\n",
            "[20]\tvalidation_0-ndcg@10:0.59675\n",
            "[40]\tvalidation_0-ndcg@10:0.59842\n",
            "[60]\tvalidation_0-ndcg@10:0.59818\n",
            "[80]\tvalidation_0-ndcg@10:0.59647\n",
            "[85]\tvalidation_0-ndcg@10:0.59693\n",
            "Best iteration: 45, best ndcg@10: 0.5996\n"
          ]
        }
      ],
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "params = {\n",
        "    'objective': 'rank:ndcg',\n",
        "    'tree_method': 'hist',\n",
        "    'learning_rate': 0.05,\n",
        "    'max_depth': 6,\n",
        "    'n_estimators': 800,\n",
        "    'random_state': 42,\n",
        "    'early_stopping_rounds': 40,\n",
        "    'eval_metric': ['ndcg@10']\n",
        "}\n",
        "\n",
        "rankers = {}\n",
        "\n",
        "for name in feature_sets:\n",
        "    print(f\"\\nTraining {name} ...\")\n",
        "\n",
        "    ranker = xgb.XGBRanker(**params)\n",
        "\n",
        "    ranker.fit(\n",
        "        X_train_dict[name],\n",
        "        y_train,\n",
        "        group=group_train,\n",
        "        eval_set=[(X_val_dict[name], y_val)],\n",
        "        eval_group=[group_val],\n",
        "        verbose=20\n",
        "    )\n",
        "\n",
        "    rankers[name] = ranker\n",
        "    print(f\"Best iteration: {ranker.best_iteration}, best ndcg@10: {ranker.best_score:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKFoFrNpLOiq",
        "outputId": "b679ee64-c07b-4ba5-eece-f736ca8db13d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Feature importances for BM25_only:\n",
            "   real_feature  importance\n",
            "0    bm25_score      0.9494\n",
            "2  query_length      0.0343\n",
            "1    doc_length      0.0164\n",
            "\n",
            "Feature importances for BERT_only:\n",
            "   real_feature  importance\n",
            "0    bert_score      0.9546\n",
            "2  query_length      0.0298\n",
            "1    doc_length      0.0157\n",
            "\n",
            "Feature importances for Combined:\n",
            "   real_feature  importance\n",
            "1    bert_score      0.8157\n",
            "0    bm25_score      0.1491\n",
            "3  query_length      0.0198\n",
            "2    doc_length      0.0154\n"
          ]
        }
      ],
      "source": [
        "# For each ranker\n",
        "for name, ranker in rankers.items():\n",
        "    print(f\"\\nFeature importances for {name}:\")\n",
        "\n",
        "    importance = pd.DataFrame({\n",
        "        'feature': [f\"f{i}\" for i in range(ranker.n_features_in_)],\n",
        "        'importance': ranker.feature_importances_\n",
        "    }).sort_values('importance', ascending=False)\n",
        "\n",
        "    # Optional: map back to your real names manually\n",
        "    real_cols = feature_sets[name]  # your dict of lists\n",
        "    importance['real_feature'] = importance['feature'].map(\n",
        "        lambda x: real_cols[int(x[1:])] if x.startswith('f') else x\n",
        "    )\n",
        "\n",
        "    print(importance[['real_feature', 'importance']].round(4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubjnq0qFR1S8",
        "outputId": "f3e3958a-66e9-42a0-fbae-7aa04b3631c1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Scoring validation queries: 100%|██████████| 518/518 [13:35<00:00,  1.57s/it]\n"
          ]
        }
      ],
      "source": [
        "# =============================================\n",
        "# Step 5: Evaluate all models on the validation set\n",
        "# Metrics: NDCG@10\n",
        "# Variants:\n",
        "#   - BM25 baseline\n",
        "#   - BERT reranker (on BM25 top-100)\n",
        "#   - BM25 + XGBoost\n",
        "#   - BERT + XGBoost\n",
        "#   - Combined (BM25 + BERT features) + XGBoost\n",
        "# =============================================\n",
        "\n",
        "import ir_measures\n",
        "from ir_measures import nDCG, R\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "# ─── Models & their feature columns ───\n",
        "models = {\n",
        "    \"BM25\":          None,   # special case — no XGBoost\n",
        "    \"BERT_rerank\":   None,\n",
        "    \"BM25_XGB\":      ['bm25_score', 'doc_length', 'query_length'],\n",
        "    \"BERT_XGB\":      ['bert_score', 'doc_length', 'query_length'],\n",
        "    \"Combined_XGB\":  ['bm25_score', 'bert_score', 'doc_length', 'query_length'],\n",
        "}\n",
        "\n",
        "# Which ranker object to use for each (None = rule-based)\n",
        "ranker_map = {\n",
        "    \"BM25\":          None,\n",
        "    \"BERT_rerank\":   None,\n",
        "    \"BM25_XGB\":      rankers[\"BM25_only\"],\n",
        "    \"BERT_XGB\":      rankers[\"BERT_only\"],\n",
        "    \"Combined_XGB\":  rankers[\"Combined\"],\n",
        "}\n",
        "\n",
        "# ─── Collect predictions ───\n",
        "val_qids = set(df_val['qid'].unique())\n",
        "val_runs = defaultdict(lambda: defaultdict(dict))   # method → qid → docid → score\n",
        "val_queries = [q for q in dataset_train.queries_iter() if q.query_id in val_qids]\n",
        "\n",
        "for query in tqdm(val_queries, desc=\"Scoring validation queries\"):\n",
        "    qid   = query.query_id\n",
        "    qtext = query.text\n",
        "    tokenized_query = qtext.lower().split()\n",
        "\n",
        "    # ─── Retrieve BM25 top-300 ───\n",
        "    bm25_scores_all = bm25.get_scores(tokenized_query)\n",
        "    top_idx = np.argsort(bm25_scores_all)[::-1][:300]\n",
        "    top_doc_ids    = [doc_ids[i] for i in top_idx]\n",
        "    top_bm25_raw   = bm25_scores_all[top_idx]\n",
        "\n",
        "    # Normalize (same as training)\n",
        "    if len(top_bm25_raw) > 1 and top_bm25_raw.max() > top_bm25_raw.min():\n",
        "        top_bm25 = (top_bm25_raw - top_bm25_raw.min()) / (top_bm25_raw.max() - top_bm25_raw.min() + 1e-8)\n",
        "    else:\n",
        "        top_bm25 = top_bm25_raw.astype(float)\n",
        "\n",
        "    # ─── BERT scores ───\n",
        "    pairs = [[qtext, doc_id_to_text[did]] for did in top_doc_ids]\n",
        "    with torch.no_grad():\n",
        "        bert_scores = reranker.predict(pairs, batch_size=32)\n",
        "\n",
        "    # ─── Build feature DataFrame ───\n",
        "    feat_df = pd.DataFrame({\n",
        "        'bm25_score':   top_bm25,\n",
        "        'bert_score':   bert_scores,\n",
        "        'doc_length':   [len(doc_id_to_text[did].split()) for did in top_doc_ids],\n",
        "        'query_length': len(tokenized_query),\n",
        "    })\n",
        "\n",
        "    # ─── Generate scores for each model ───\n",
        "    for method, cols in models.items():\n",
        "        if method in [\"BM25\", \"BERT_rerank\"]:\n",
        "            # rule-based\n",
        "            scores = feat_df['bm25_score'].values if method == \"BM25\" else feat_df['bert_score'].values\n",
        "        else:\n",
        "            # XGBoost model\n",
        "            X_val_pred = feat_df[cols].values\n",
        "            scores = ranker_map[method].predict(X_val_pred)\n",
        "\n",
        "        # Store\n",
        "        for i, did in enumerate(top_doc_ids):\n",
        "            val_runs[method][qid][did] = float(scores[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ZX6k30BchUxi",
        "outputId": "256760b0-23e3-4656-c588-23de434204b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation results:\n",
            "\n",
            "BM25              nDCG@10  0.2630   R@100    0.1988  \n",
            "BERT_rerank       nDCG@10  0.3099   R@100    0.2192  \n",
            "BM25_XGB          nDCG@10  0.2643   R@100    0.2011  \n",
            "BERT_XGB          nDCG@10  0.3114   R@100    0.2196  \n",
            "Combined_XGB      nDCG@10  0.3092   R@100    0.2190  \n"
          ]
        }
      ],
      "source": [
        "# ─────────────────────────────────────────────\n",
        "# Evaluate all runs\n",
        "# ─────────────────────────────────────────────\n",
        "\n",
        "metrics = [nDCG@10, R@100]\n",
        "\n",
        "print(\"\\nValidation results:\\n\")\n",
        "\n",
        "for method_name in val_runs:\n",
        "    flat_run = [\n",
        "        ir_measures.ScoredDoc(qid, did, score)\n",
        "        for qid, docs in val_runs[method_name].items()\n",
        "        for did, score in docs.items()\n",
        "    ]\n",
        "\n",
        "    agg_results = ir_measures.calc_aggregate(metrics, val_qrels, flat_run)\n",
        "\n",
        "    print(f\"{method_name:16}\", end=\" \")\n",
        "    for m in metrics:\n",
        "        print(f\" {str(m):8} {agg_results[m]:.4f}\", end=\"  \")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wcylkme_2rkP",
        "outputId": "f8f8e0a0-0515-41b5-a8cd-f51db00c43e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Final evaluation on beir/nfcorpus/test ===\n",
            "\n",
            "Test queries: 323 | Test judgments: 12334\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Scoring test queries: 100%|██████████| 323/323 [08:28<00:00,  1.58s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test results:\n",
            "\n",
            "BM25              nDCG@10    0.2613   R@100      0.2027  \n",
            "BERT_rerank       nDCG@10    0.3161   R@100      0.2200  \n",
            "BM25_XGB          nDCG@10    0.2610   R@100      0.2002  \n",
            "BERT_XGB          nDCG@10    0.3132   R@100      0.2180  \n",
            "Combined_XGB      nDCG@10    0.3088   R@100      0.2222  \n"
          ]
        }
      ],
      "source": [
        "# =============================================\n",
        "# FINAL EVALUATION ON OFFICIAL TEST SPLIT\n",
        "# =============================================\n",
        "\n",
        "import ir_measures\n",
        "from ir_measures import nDCG, R\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "metrics = [nDCG@10, R@100]\n",
        "\n",
        "print(\"\\n=== Final evaluation on beir/nfcorpus/test ===\\n\")\n",
        "\n",
        "test_qrels = list(dataset_test.qrels_iter())\n",
        "test_queries = list(dataset_test.queries_iter())\n",
        "\n",
        "for qrel in dataset_test.qrels_iter():\n",
        "    qid = qrel.query_id\n",
        "    did = qrel.doc_id\n",
        "    rel = qrel.relevance  # usually 0/1/2\n",
        "    if qid not in qrels_dict:\n",
        "        qrels_dict[qid] = {}\n",
        "    qrels_dict[qid][did] = rel\n",
        "\n",
        "test_queries = list(dataset_test.queries_iter())\n",
        "\n",
        "print(f\"Test queries: {len(test_queries)} | Test judgments: {len(test_qrels)}\")\n",
        "\n",
        "# ─── Define models and ranker mapping ───\n",
        "models = {\n",
        "    \"BM25\":          None,\n",
        "    \"BERT_rerank\":   None,\n",
        "    \"BM25_XGB\":      ['bm25_score', 'doc_length', 'query_length'],\n",
        "    \"BERT_XGB\":      ['bert_score', 'doc_length', 'query_length'],\n",
        "    \"Combined_XGB\":  ['bm25_score', 'bert_score', 'doc_length', 'query_length'],\n",
        "}\n",
        "\n",
        "ranker_map = {\n",
        "    \"BM25\":          None,\n",
        "    \"BERT_rerank\":   None,\n",
        "    \"BM25_XGB\":      rankers.get(\"BM25_only\"),\n",
        "    \"BERT_XGB\":      rankers.get(\"BERT_only\"),\n",
        "    \"Combined_XGB\":  rankers.get(\"Combined\"),\n",
        "}\n",
        "\n",
        "test_runs = defaultdict(lambda: defaultdict(dict))\n",
        "\n",
        "for query in tqdm(test_queries, desc=\"Scoring test queries\"):\n",
        "    qid   = query.query_id\n",
        "    qtext = query.text\n",
        "    tokenized_query = qtext.lower().split()\n",
        "\n",
        "    # Retrieve BM25 top-100\n",
        "    bm25_scores_all = bm25.get_scores(tokenized_query)\n",
        "    top_idx = np.argsort(bm25_scores_all)[::-1][:300]\n",
        "    top_doc_ids    = [doc_ids[i] for i in top_idx]\n",
        "    top_bm25_raw   = bm25_scores_all[top_idx]\n",
        "\n",
        "    # Normalize\n",
        "    if len(top_bm25_raw) > 1 and top_bm25_raw.max() > top_bm25_raw.min():\n",
        "        top_bm25 = (top_bm25_raw - top_bm25_raw.min()) / (top_bm25_raw.max() - top_bm25_raw.min() + 1e-8)\n",
        "    else:\n",
        "        top_bm25 = top_bm25_raw.astype(float)\n",
        "\n",
        "    # BERT scores\n",
        "    pairs = [[qtext, doc_id_to_text[did]] for did in top_doc_ids]\n",
        "    with torch.no_grad():\n",
        "        bert_scores = reranker.predict(pairs, batch_size=32)\n",
        "\n",
        "    # Features\n",
        "    feat_df = pd.DataFrame({\n",
        "        'bm25_score':   top_bm25,\n",
        "        'bert_score':   bert_scores,\n",
        "        'doc_length':   [len(doc_id_to_text[did].split()) for did in top_doc_ids],\n",
        "        'query_length': len(tokenized_query),\n",
        "    })\n",
        "\n",
        "    # Score each model\n",
        "    for method, cols in models.items():\n",
        "        if method in [\"BM25\", \"BERT_rerank\"]:\n",
        "            scores = feat_df['bm25_score'].values if method == \"BM25\" else feat_df['bert_score'].values\n",
        "        else:\n",
        "            if ranker_map[method] is None:\n",
        "                continue  # skip if no ranker trained for this variant\n",
        "            X_test_pred = feat_df[cols].values\n",
        "            scores = ranker_map[method].predict(X_test_pred)\n",
        "\n",
        "        for i, did in enumerate(top_doc_ids):\n",
        "            test_runs[method][qid][did] = float(scores[i])\n",
        "\n",
        "# ─── Evaluate ───\n",
        "print(\"\\nTest results:\\n\")\n",
        "\n",
        "for method_name in test_runs:\n",
        "    flat_run = [\n",
        "        ir_measures.ScoredDoc(qid, did, score)\n",
        "        for qid, docs in test_runs[method_name].items()\n",
        "        for did, score in docs.items()\n",
        "    ]\n",
        "\n",
        "    if not flat_run:\n",
        "        print(f\"{method_name:16} No predictions generated\")\n",
        "        continue\n",
        "\n",
        "    agg_results = ir_measures.calc_aggregate(metrics, test_qrels, flat_run)\n",
        "\n",
        "    print(f\"{method_name:16}\", end=\" \")\n",
        "    for m in metrics:\n",
        "        print(f\" {str(m):10} {agg_results[m]:.4f}\", end=\"  \")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "621b6c1ef13f4ee38d2e35d7277b2ac5",
            "166d8fd103ed4498b9d82130fa20a0c6",
            "7363e84c811441d9bc11307e72dac210",
            "436f87194cf04cfb9c8334d888fba511",
            "dcfec4cb6a1a4d29baaa17da660d8a0d",
            "c0f5ec9754384f928cbb7438a8fbf268",
            "188f2620840d4210ac5658efbe794ac9",
            "5474af807167430d9e6411b980d82a2c",
            "b1d9c4c0e77b4f5291c72b4347e3177e",
            "1ce5bc617d664eab90a7b5c51182f478",
            "a3cba83a560f4699bdd73d78105bc497"
          ]
        },
        "id": "wZ75E_BOmX1O",
        "outputId": "275f6741-4937-4345-e800-5f6b8f8802c0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "621b6c1ef13f4ee38d2e35d7277b2ac5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BERT reranker saved to: saved_models/bert_reranker_ms_marco_mini_lm\n",
            "Baseline BM25 saved.\n",
            "XGBoost model saved.\n"
          ]
        }
      ],
      "source": [
        "# ─────────────────────────────────────────────\n",
        "# Save the models\n",
        "# ─────────────────────────────────────────────\n",
        "\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "reranker.save_pretrained(\"saved_models/bert_reranker_ms_marco_mini_lm\")\n",
        "print(\"BERT reranker saved to: saved_models/bert_reranker_ms_marco_mini_lm\")\n",
        "\n",
        "with open(\"saved_models/bm25_index.pkl\", \"wb\") as f:\n",
        "    pickle.dump(bm25, f)\n",
        "with open(\"saved_models/corpus_dict.pkl\", \"wb\") as f:\n",
        "    pickle.dump(doc_id_to_text, f)\n",
        "print(\"Baseline BM25 saved.\")\n",
        "\n",
        "rankers[\"BM25_only\"].save_model(\"saved_models/bm25_xgb.json\")\n",
        "rankers[\"BERT_only\"].save_model(\"saved_models/bert_xgb.json\")\n",
        "rankers[\"Combined\"].save_model(\"saved_models/combined_xgb.json\")\n",
        "print(\"XGBoost model saved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJ3yE9p9pcWL",
        "outputId": "52e5f8c2-4dd5-402c-84bc-8bbf954faaae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "updating: saved_models/ (stored 0%)\n",
            "updating: saved_models/combined_xgb.json (deflated 75%)\n",
            "updating: saved_models/corpus_dict.pkl (deflated 66%)\n",
            "updating: saved_models/bm25_xgb.json (deflated 74%)\n",
            "updating: saved_models/bert_xgb.json (deflated 75%)\n",
            "updating: saved_models/bm25_index.pkl (deflated 68%)\n",
            "updating: saved_models/bert_reranker_ms_marco_mini_lm/ (stored 0%)\n",
            "updating: saved_models/bert_reranker_ms_marco_mini_lm/config.json (deflated 52%)\n",
            "updating: saved_models/bert_reranker_ms_marco_mini_lm/model.safetensors (deflated 8%)\n",
            "updating: saved_models/bert_reranker_ms_marco_mini_lm/tokenizer_config.json (deflated 47%)\n",
            "updating: saved_models/bert_reranker_ms_marco_mini_lm/tokenizer.json (deflated 71%)\n",
            "updating: saved_models/bert_reranker_ms_marco_mini_lm/README.md (deflated 58%)\n"
          ]
        }
      ],
      "source": [
        "!zip -r saved_models.zip saved_models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "ru4jmNBQpemZ",
        "outputId": "5d14b51e-f010-4fae-b41c-8531969d3750"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_52c42450-369f-49db-9047-8f72f97e2940\", \"saved_models.zip\", 87736130)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.download(\"saved_models.zip\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "166d8fd103ed4498b9d82130fa20a0c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0f5ec9754384f928cbb7438a8fbf268",
            "placeholder": "​",
            "style": "IPY_MODEL_188f2620840d4210ac5658efbe794ac9",
            "value": "Writing model shards: 100%"
          }
        },
        "188f2620840d4210ac5658efbe794ac9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a85fde2621e46f5b3d629438a229a66": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aff095f64c42451ebf8e602d866a98eb",
            "max": 105,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8f6a93e812f24acda0b3e503b8f3913b",
            "value": 105
          }
        },
        "1ce5bc617d664eab90a7b5c51182f478": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "436f87194cf04cfb9c8334d888fba511": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ce5bc617d664eab90a7b5c51182f478",
            "placeholder": "​",
            "style": "IPY_MODEL_a3cba83a560f4699bdd73d78105bc497",
            "value": " 1/1 [00:00&lt;00:00,  4.93it/s]"
          }
        },
        "46445ad693d14885a7be60c0ba12316d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5474af807167430d9e6411b980d82a2c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "621b6c1ef13f4ee38d2e35d7277b2ac5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_166d8fd103ed4498b9d82130fa20a0c6",
              "IPY_MODEL_7363e84c811441d9bc11307e72dac210",
              "IPY_MODEL_436f87194cf04cfb9c8334d888fba511"
            ],
            "layout": "IPY_MODEL_dcfec4cb6a1a4d29baaa17da660d8a0d"
          }
        },
        "7363e84c811441d9bc11307e72dac210": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5474af807167430d9e6411b980d82a2c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b1d9c4c0e77b4f5291c72b4347e3177e",
            "value": 1
          }
        },
        "8f6a93e812f24acda0b3e503b8f3913b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "976989d10af04dd2afaf6a6e358a915c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "97e9a6992d624806bb8ab6fde93cb09a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f23457510032417c932208452e88c6dc",
              "IPY_MODEL_1a85fde2621e46f5b3d629438a229a66",
              "IPY_MODEL_b7407824d18f40f5b6adefb6b320bc77"
            ],
            "layout": "IPY_MODEL_ceceac60e77145fca5d6d1a852816c7d"
          }
        },
        "a3cba83a560f4699bdd73d78105bc497": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aff095f64c42451ebf8e602d866a98eb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1d9c4c0e77b4f5291c72b4347e3177e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b2fde80be0e44675a765c9df2c0c3388": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7407824d18f40f5b6adefb6b320bc77": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46445ad693d14885a7be60c0ba12316d",
            "placeholder": "​",
            "style": "IPY_MODEL_b95d72ffe9a045afb8a57c6204a42cb1",
            "value": " 105/105 [00:00&lt;00:00, 532.32it/s, Materializing param=classifier.weight]"
          }
        },
        "b95d72ffe9a045afb8a57c6204a42cb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c0f5ec9754384f928cbb7438a8fbf268": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ceceac60e77145fca5d6d1a852816c7d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dcfec4cb6a1a4d29baaa17da660d8a0d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f23457510032417c932208452e88c6dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2fde80be0e44675a765c9df2c0c3388",
            "placeholder": "​",
            "style": "IPY_MODEL_976989d10af04dd2afaf6a6e358a915c",
            "value": "Loading weights: 100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
